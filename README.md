# Neural-machine-translation
Machine Translation is an application of NLP where one Language is translated into another language. Example translating Spanish to English. Machine Translation using Neural networks especially Recurrent models, is called Neural Machine Translation or in short NMT. Most widely used Deep Learning model for NMT is seq2seq model which has Encoder and Decoder. At a high-level Encoder takes input sentence and Decoder outputs translated target sentence. Both Encoder and Decoder models are build using LSTM layers .This dataset contains a pair of sentences with Spanish and English. Each line has an input sentence and a target sentence separated by a tab space(“\t”). We will be using this data for training, validation, and testing. The first impression created by looking at the data, makes us think we should be able to extract each line iteratively and split each line by tab separation. Save these tab separated sentences in a list corresponding to its language(input or target).
Part 2: MODEL
Model building is a very simple process in Keras. It involves the following steps:
Encoder:
The encoder is constructed with an input layer, LSTM layer. For encoder LSTM return_state is set to True, return_seq is set to False. Since we do not need output at every time step we make return_seq=False. We need the last hidden state to be passed as hidden input to the decoder we make return_state=True. For detailed information about return_state and return_sequences refer to this blog.
Decoder:
The decoder has an Input layer, LSTM layer, and Dense layer followed by a Softmax. Input is the character-wise target sentence, at the output time step we have to predict the next character. Hence we need softmax to predict the next character. In the decoder, both return_state and return_seq are set to True. Because we need decoder output at every time step to predict the next character with the help of return_seq=True. We take the hidden input and feed it to the next step explicitly during inference with the help of return_state=True. Hidden state output is not needed during training, it is needed only during inference.
